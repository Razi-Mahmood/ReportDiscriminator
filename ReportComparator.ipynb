{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "intimate-airfare",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "clinical-graph",
   "metadata": {},
   "source": [
    "### Experiments with Sentence Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "controversial-weight",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "spiritual-spare",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = SentenceTransformer('paraphrase-distilroberta-base-v1') #we can use robert or miniLM\n",
    "model=SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "directed-equilibrium",
   "metadata": {},
   "outputs": [],
   "source": [
    "report_file_path = \"../reportscoring/\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "breathing-ecuador",
   "metadata": {},
   "source": [
    "### QI score Calculations to measure report quality improvement\n",
    "### Using the SBERT method for encoding the reports captures semantics better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d3f16908",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# reads the file, extracts sentences, encodes reports, converts them to a vector\n",
    "def report_encoder(filepath, model):\n",
    "\n",
    "    #file = open(filepath)\n",
    "    file = open(filepath, 'r')\n",
    "    Lines = file.readlines()\n",
    "    #extract sentences\n",
    "    allsentences=[]\n",
    "    #sort the sentences lexically\n",
    "    for perline in Lines:\n",
    "        if (perline.strip()!=\"\"):\n",
    "            linetokens=perline.split(\".\")\n",
    "            for line in linetokens:\n",
    "                if (line.strip()!=\"\"):\n",
    "                    line1 = line.strip()+\".\"\n",
    "                    line1=line1.lower()\n",
    "                    allsentences.append(line1)\n",
    "       \n",
    "    file.close()\n",
    "    #not necessary since we are averaging across the sentences\n",
    "    sortedsentences=sorted(allsentences)\n",
    "   # print(len(allsentences))\n",
    "    #print(allsentences)\n",
    "    #encode the report\n",
    "    sentence_embeddings = model.encode(sortedsentences)\n",
    "    #print(sentence_embeddings)\n",
    "    \n",
    "    sentence_avg = np.average(sentence_embeddings, axis = 0)\n",
    "    #print(sentence_avg.shape)\n",
    "    #convert them into a single vector\n",
    "    sentence_avg = sentence_avg.reshape(1, sentence_avg.shape[0])\n",
    "    #print(sentence_avg)\n",
    "    \n",
    "    return sentence_avg\n",
    "    \n",
    "def compute_QI_score_BERT(reportdir,model):\n",
    "    origdir=reportdir+\"origreports\"\n",
    "    aidir=reportdir+\"aireports\"\n",
    "    modifdir=reportdir+\"correctedreports\"\n",
    "    filenames=[]\n",
    "    ntotal=0\n",
    "    npositive=0\n",
    "    nnegative=0\n",
    "    nequal=0\n",
    "    for root, dirs, files in os.walk(modifdir, topdown=False, onerror=None, followlinks=True):\n",
    "        for filename in files:\n",
    "            if filename != '.DS_Store':\n",
    "                filepath = os.path.join(root, filename)\n",
    "                filenames.append(filename)\n",
    "    for filename in filenames:\n",
    "        origpath=os.path.join(origdir,filename)\n",
    "        aipath=os.path.join(aidir,filename)\n",
    "        modifpath=os.path.join(modifdir,filename)\n",
    "        \n",
    "        vect_orig=report_encoder(origpath,model)\n",
    "       # print(\"Ai report\")\n",
    "        vect_ai=report_encoder(aipath,model)\n",
    "        #print(\"Modif report\")\n",
    "        vect_modif=report_encoder(modifpath, model)\n",
    "        \n",
    "        D12=cosine_similarity([vect_orig[0],vect_ai[0]])\n",
    "        D13=cosine_similarity([vect_orig[0],vect_modif[0]])\n",
    "        orig_ai_score=D12[0,1]\n",
    "        orig_modif_score=D13[0,1]\n",
    "        ntotal+=1\n",
    "       # print(orig_modif_score,orig_ai_score)\n",
    "        if (orig_modif_score>orig_ai_score):\n",
    "            npositive+=1\n",
    "        elif (orig_modif_score<orig_ai_score):\n",
    "            nnegative+=1\n",
    "        else:\n",
    "            nequal+=1\n",
    "    QI_score=((npositive+nequal-nnegative)/ntotal)*100\n",
    "    print(npositive,ntotal,(npositive/ntotal),(nnegative/ntotal), (nequal/ntotal))\n",
    "    print(\"Net improvement = \",QI_score,\"%\")\n",
    "    return QI_score,npositive,nnegative,nequal,ntotal\n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "f0843c26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1105 3661 0.3018301010652827 0.4228352909041246 0.27533460803059273\n",
      "Net improvement =  15.432941819175088\n"
     ]
    }
   ],
   "source": [
    "qi_score,npositive,nnegative,nequal,ntotal=compute_QI_score_BERT(report_file_path,model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6af70975",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.93675435\n",
      "0.0105115714765347\n"
     ]
    }
   ],
   "source": [
    "Sent1=\"There is no pleural effusion.\"\n",
    "Sent2=\"There is no evidence of pleural effusion.\"\n",
    "e1= model.encode(Sent1)\n",
    "e2=model.encode(Sent2)\n",
    "D12=cosine_similarity([e1,e2])\n",
    "print(D12[0,1])\n",
    "print(bleu(Sent1,Sent2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "refined-recruitment",
   "metadata": {},
   "source": [
    "\n",
    "## Quality improvement using BLEU score for capturing the similarity between report sentences\n",
    "### truncates the report to have the same sentences as BLEU works with equal number of sentences in the report\n",
    "##### reference: https://towardsdatascience.com/how-to-evaluate-text-generation-models-metrics-for-automatic-evaluation-of-nlp-models-e1c251b04ec1\n",
    "#### WIth Bleu score we get much higher quality improvement\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "suspected-blair",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import SmoothingFunction, corpus_bleu, sentence_bleu\n",
    "\n",
    "def bleu_encode(filepath):\n",
    "\n",
    "    #file = open(filepath)\n",
    "    file = open(filepath, 'r')\n",
    "    Lines = file.readlines()\n",
    "    #extract sentences\n",
    "    allsentences=[]\n",
    "    #sort the sentences lexically\n",
    "    for perline in Lines:\n",
    "        if (perline.strip()!=\"\"):\n",
    "            linetokens=perline.split(\".\")\n",
    "            for line in linetokens:\n",
    "                if (line.strip()!=\"\"):\n",
    "                    line1 = line.strip()+\".\"\n",
    "                    line1=line1.lower()\n",
    "                    allsentences.append(line1)\n",
    "       \n",
    "    file.close()\n",
    "    #not necessary since we are averaging across the sentences\n",
    "    sortedsentences=sorted(allsentences)\n",
    "   \n",
    "    return sortedsentences\n",
    "    \n",
    "def bleu(rep1, rep2):\n",
    "    ''' \n",
    "    calculate pair wise bleu score. uses nltk implementation\n",
    "    Args:\n",
    "        references : a list of reference sentences \n",
    "        candidates : a list of candidate(generated) sentences\n",
    "    Returns:\n",
    "        bleu score(float)\n",
    "    '''\n",
    "    rep1_bleu = []\n",
    "    rep2_bleu = []\n",
    "    for l in rep2:\n",
    "        rep2_bleu.append(l.split())\n",
    "        \n",
    "    for i,l in enumerate(rep1):\n",
    "        rep1_bleu.append([l.split()])\n",
    "    cc = SmoothingFunction()\n",
    "    minlength=min(len(rep1_bleu), len(rep2_bleu))\n",
    "   # print(minlength,len(rep1_bleu), len(rep2_bleu))\n",
    "    score_bleu = corpus_bleu(rep1_bleu[0:minlength], rep2_bleu[0:minlength], weights=(0, 1, 0, 0), smoothing_function=cc.method4)\n",
    "    return score_bleu\n",
    "\n",
    "def compute_QI_score_BLEU(reportdir):\n",
    "    origdir=reportdir+\"origreports\"\n",
    "    aidir=reportdir+\"aireports\"\n",
    "    modifdir=reportdir+\"correctedreports\"\n",
    "    filenames=[]\n",
    "    ntotal=0\n",
    "    npositive=0\n",
    "    nnegative=0\n",
    "    nequal=0\n",
    "    for root, dirs, files in os.walk(modifdir, topdown=False, onerror=None, followlinks=True):\n",
    "        for filename in files:\n",
    "            if filename != '.DS_Store':\n",
    "                filepath = os.path.join(root, filename)\n",
    "                filenames.append(filename)\n",
    "    for filename in filenames:\n",
    "        origpath=os.path.join(origdir,filename)\n",
    "        aipath=os.path.join(aidir,filename)\n",
    "        modifpath=os.path.join(modifdir,filename)\n",
    "        origsent= bleu_encode(origpath)\n",
    "        aisent=bleu_encode(aipath)\n",
    "        modifsent=bleu_encode(modifpath)\n",
    "       \n",
    "        orig_ai_score=bleu(origsent,aisent)\n",
    "        orig_modif_score=bleu(origsent,modifsent)\n",
    "        ntotal+=1\n",
    "       # print(orig_modif_score,orig_ai_score)\n",
    "        if (orig_modif_score>orig_ai_score):\n",
    "            npositive+=1\n",
    "        elif (orig_modif_score<orig_ai_score):\n",
    "            nnegative+=1\n",
    "        else:\n",
    "            nequal+=1\n",
    "    QI_score=((npositive+nequal-nnegative)/ntotal)*100\n",
    "    print(npositive,ntotal,(npositive/ntotal),(nnegative/ntotal), (nequal/ntotal))\n",
    "    print(\"Net improvement = \",QI_score,\"%\")\n",
    "    return QI_score,npositive,nnegative,nequal,ntotal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "80383973",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1312 3661 0.3583720295001366 0.2838022398251844 0.35782573067467904\n",
      "Net improvement =  43.239552034963125 %\n"
     ]
    }
   ],
   "source": [
    "QI_score,npositive,nnegative,nequal,ntotal=compute_QI_score_BLEU(report_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7bd850d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "envpytorch",
   "language": "python",
   "name": "envpytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
